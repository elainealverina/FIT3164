{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 18,
            "source": [
                "import matplotlib.pyplot as plt\r\n",
                "import numpy as np\r\n",
                "import torch\r\n",
                "import pandas as pd\r\n",
                "\r\n",
                "from torch import nn\r\n",
                "from torch import optim\r\n",
                "import torch.nn.functional as F\r\n",
                "from torch.utils.data import Dataset, DataLoader\r\n",
                "from torchvision import datasets, transforms, models\r\n",
                "from PIL import Image\r\n",
                "from torch.autograd import Variable\r\n",
                "from typing import Union\r\n",
                "from PIL import Image\r\n",
                "from torch import Tensor"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# root_dir_test = '/Users/vionnietan/Desktop/trial_dataset/coad_msi_mss/test'\r\n",
                "root_dir_test = 'C:/Users/jones/Desktop/cancer dataset/coad_msi_mss/test'\r\n",
                "data_transformation_test = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\r\n",
                "\r\n",
                "test_image_dataset = datasets.ImageFolder(root = root_dir_test, transform=data_transformation_test)\r\n",
                "testloader = DataLoader(test_image_dataset, batch_size=128, shuffle=True)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
                "#model = torch.load('/Users/vionnietan/Desktop/resnet18.pth')\r\n",
                "model = torch.load('C:/Users/jones/Desktop/FIT3164/22septmodel.pth')\r\n",
                "model.eval()\r\n",
                "model"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "class_names = testloader.dataset.classes\r\n",
                "\r\n",
                "def show_images(images, labels, preds):\r\n",
                "    plt.figure(figsize=(8,4))\r\n",
                "    for i, image in enumerate(images):\r\n",
                "        if i < 5:\r\n",
                "            plt.subplot(1, 6, i+1, xticks=[], yticks=[])\r\n",
                "            image = image.numpy().transpose((1, 2, 0))\r\n",
                "            \r\n",
                "            mean = np.array([0.485, 0.456, 0.406])\r\n",
                "            std = np.array([0.229, 0.224, 0.225])\r\n",
                "            \r\n",
                "            image = image*std + mean\r\n",
                "            image = np.clip(image, 0.,1.)\r\n",
                "            plt.imshow(image)\r\n",
                "            \r\n",
                "            colour = 'green' if preds[i] == labels[i] else 'red'\r\n",
                "            plt.xlabel(f'{class_names[int(labels[i].numpy())]}')\r\n",
                "            plt.ylabel(f'{class_names[int(preds[i].numpy())]}', color=colour)\r\n",
                "    plt.tight_layout()        \r\n",
                "    plt.show()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def show_preds():\r\n",
                "    model.eval()    # set to evaluation mode\r\n",
                "    images, labels = next(iter(testloader))\r\n",
                "    outputs = model(images)\r\n",
                "    _ , preds = torch.max(outputs, 1)\r\n",
                "    #print(labels)\r\n",
                "    show_images(images, labels, preds)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "show_preds()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "def to_numpy(tensor: Union[Tensor, Image.Image, np.array]) -> np.ndarray:\r\n",
                "    if type(tensor) == np.array or type(tensor) == np.ndarray:\r\n",
                "        return np.array(tensor)\r\n",
                "    elif type(tensor) == Image.Image:\r\n",
                "        return np.array(tensor)\r\n",
                "    elif type(tensor) == Tensor:\r\n",
                "        return tensor.cpu().detach().numpy()\r\n",
                "    else:\r\n",
                "        raise ValueError()\r\n",
                "\r\n",
                "from sklearn.metrics import confusion_matrix\r\n",
                "from sklearn.metrics import roc_auc_score\r\n",
                "\r\n",
                "def test_label_predictions(model, device, test_loader):\r\n",
                "    model.eval()\r\n",
                "    actuals = []\r\n",
                "    predictions = []\r\n",
                "    with torch.no_grad():\r\n",
                "        for inputs, labels in test_loader:\r\n",
                "            inputs, labels = inputs.to(device), labels.to(device)\r\n",
                "            outputs = model(inputs)\r\n",
                "            prediction = outputs.argmax(dim=1, keepdim=True)\r\n",
                "            actuals.extend(to_numpy(labels.view_as(prediction)))\r\n",
                "            predictions.extend(to_numpy(prediction))\r\n",
                "            \r\n",
                "    return [i.item() for i in actuals], [i.item() for i in predictions]\r\n",
                "\r\n",
                "actuals, predictions = test_label_predictions(model, device, testloader)\r\n",
                "print('Confusion matrix for resnet18: ')\r\n",
                "print(confusion_matrix(actuals, predictions))\r\n",
                "print('AUC score for model resnet18: '+str(roc_auc_score(actuals,predictions)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Confusion matrix for resnet18: \n",
                        "[[224 210]\n",
                        " [129 523]]\n",
                        "AUC score for model resnet18: 0.659138135760934\n"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.8",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.8 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "ebf8d43d3d173dc48345ffb6b59ad16b10ee255e28de3a129e6f6400e8327d9f"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}